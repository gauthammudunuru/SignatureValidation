import base64
from io import BytesIO
import matplotlib.pyplot as plt
from openai import OpenAI

# Your OpenAI client
client = OpenAI()

# Assume cropped_images is a list of PIL Image objects from your rectangles
classified_results = []

handwritten = []
printed = []
non_text = []

def show_image(img, title=""):
    plt.figure(figsize=(6, 8))
    plt.imshow(img)
    plt.axis("off")
    if title:
        plt.title(title)
    plt.show()

for idx, img in enumerate(cropped_images):
    try:
        # Convert image to base64
        buffered = BytesIO()
        img.save(buffered, format="PNG")
        img_b64 = base64.b64encode(buffered.getvalue()).decode("utf-8")
        
        # Build prompt
        prompt = (
            "You are a document image classifier. "
            "The following is a PNG image encoded in base64. "
            "Classify it into one of the following categories: "
            "Printed text, Handwritten content, Non-text image. "
            "Reply only with the category name.\n\n"
            f"Image(base64): {img_b64}"
        )
        
        # Call Qwen3-omni
        response = client.chat.completion.create(
            model="Qwen3-omni",
            messages=[
                {"role": "system", "content": "You are a helpful assistant for classifying document image crops."},
                {"role": "user", "content": prompt}
            ]
        )
        
        # Extract classification
        classification = response.choices[0].message.content.strip()
        print(f"Crop {idx+1}: {classification}")
        
        # Display the image inline for visual check
        show_image(img, f"Crop {idx+1}: {classification}")
        
        # Store results
        classified_results.append({
            "index": idx,
            "image": img,
            "classification": classification
        })
        
        # Separate into categories
        cat = classification.lower()
        if "handwritten" in cat:
            handwritten.append(img)
        elif "printed" in cat:
            printed.append(img)
        else:
            non_text.append(img)
    
    except Exception as e:
        print(f"Crop {idx+1} skipped due to error: {e}")
        continue

print(f"âœ… Finished processing {len(classified_results)} crops successfully.")
print(f"Handwritten: {len(handwritten)}, Printed: {len(printed)}, Non-text: {len(non_text)}")



import matplotlib.pyplot as plt

def show_images_grid(images, title="", cols=3):
    if not images:
        return
    rows = (len(images) + cols - 1) // cols
    plt.figure(figsize=(5*cols, 5*rows))
    for i, img in enumerate(images):
        plt.subplot(rows, cols, i+1)
        plt.imshow(img)
        plt.axis("off")
    if title:
        plt.suptitle(title, fontsize=16)
    plt.show()

# After classification loop
show_images_grid(handwritten, "Handwritten Crops")
show_images_grid(printed, "Printed Text Crops")
show_images_grid(non_text, "Non-Text / Image Crops")





max_dim = 1024

def preprocess_for_llm(img):
    w, h = img.size
    scale = min(max_dim/w, max_dim/h, 1.0)
    img_resized = img.resize((int(w*scale), int(h*scale)), Image.LANCZOS)
    gray = cv2.cvtColor(np.array(img_resized), cv2.COLOR_RGB2GRAY)
    edges = cv2.Canny(gray, 50, 150)
    edge_density = np.sum(edges)/(img_resized.width*img_resized.height)
    return img_resized, edge_density

# Filter crops with minimal info
filtered_crops = []
for img in cropped_images:
    img_resized, density = preprocess_for_llm(img)
    if density > 0.01:  # threshold can be tuned
        filtered_crops.append(img_resized)
